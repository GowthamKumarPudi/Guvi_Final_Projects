{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OqRjqJoaQnID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1f598cdf-5c12-4413-9559-8b0408f2c3d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Shape: (33248, 21)\n",
            "Test Shape: (22166, 20)\n",
            "\n",
            "Missing values in Train:\n",
            " Director_Score      0.659408\n",
            "Profile             0.634835\n",
            "Director            0.625150\n",
            "Remote              0.581058\n",
            "Revenue             0.550950\n",
            "URL                 0.482224\n",
            "Employee            0.384955\n",
            "Reviews             0.263535\n",
            "Company_Score       0.263535\n",
            "Sector_Group        0.216975\n",
            "Sector              0.216975\n",
            "City                0.115014\n",
            "State               0.093600\n",
            "Location            0.000391\n",
            "Company             0.000271\n",
            "ID                  0.000000\n",
            "Jobs_Group          0.000000\n",
            "Job                 0.000000\n",
            "Skills              0.000000\n",
            "Frecuency_Salary    0.000000\n",
            "Mean_Salary         0.000000\n",
            "dtype: float64\n",
            "\n",
            "Missing values in Test:\n",
            " Director_Score      0.652531\n",
            "Profile             0.634350\n",
            "Director            0.616304\n",
            "Remote              0.578453\n",
            "Revenue             0.542362\n",
            "URL                 0.474556\n",
            "Employee            0.379771\n",
            "Company_Score       0.262925\n",
            "Reviews             0.262925\n",
            "Sector              0.213615\n",
            "Sector_Group        0.213615\n",
            "City                0.118425\n",
            "State               0.095597\n",
            "Company             0.000451\n",
            "Location            0.000361\n",
            "ID                  0.000000\n",
            "Jobs_Group          0.000000\n",
            "Job                 0.000000\n",
            "Skills              0.000000\n",
            "Frecuency_Salary    0.000000\n",
            "dtype: float64\n",
            "\n",
            "Train Sample:\n",
            "                      ID                                                Job  \\\n",
            "0  job_f2c807527f687b96  Part-time Reporting Business Analyst, Data & A...   \n",
            "1  job_2660d4c53505af10                                         Controller   \n",
            "2   sj_50358c44328ae06a                                 Sr Finance Analyst   \n",
            "\n",
            "          Jobs_Group Profile  Remote                              Company  \\\n",
            "0  Financial Analyst     NaN  Remote        Sandy Hook Promise Foundation   \n",
            "1         Controller     NaN     NaN  Building Service 32BJ Benefit Funds   \n",
            "2  Financial Analyst  Senior     NaN                                  LCS   \n",
            "\n",
            "                            Location      City State Frecuency_Salary  ...  \\\n",
            "0                             Remote       NaN   NaN             hour  ...   \n",
            "1  New York, NY 10013Â (Tribeca area)  New York    NY             year  ...   \n",
            "2                                NaN       NaN   NaN             year  ...   \n",
            "\n",
            "                                              Skills  \\\n",
            "0                         ['Salesforce', 'Bachelor']   \n",
            "1  ['SQL', 'Master', 'Dynamics 365', 'Snowflake',...   \n",
            "2                      ['Word', 'Bachelor', 'Excel']   \n",
            "\n",
            "                             Sector             Sector_Group Revenue Employee  \\\n",
            "0  NGOs and Nonprofit Organizations  Nonprofit Organizations     NaN       XS   \n",
            "1  NGOs and Nonprofit Organizations  Nonprofit Organizations     NaN        M   \n",
            "2        Personal Consumer Services                    Sales    XXXS     XXXS   \n",
            "\n",
            "  Company_Score  Reviews                              Director Director_Score  \\\n",
            "0           4.2     20.0                                   NaN            NaN   \n",
            "1           3.5     58.0  Peter Goldberger, Executive Director            0.7   \n",
            "2           3.4     88.0                                   NaN            NaN   \n",
            "\n",
            "                                 URL  \n",
            "0  https://www.sandyhookpromise.org/  \n",
            "1                                NaN  \n",
            "2                                NaN  \n",
            "\n",
            "[3 rows x 21 columns]\n",
            "\n",
            "Test Sample:\n",
            "                      ID                                                Job  \\\n",
            "0   sj_99ad4f80ae7f4835                           Business Analyst Manager   \n",
            "1  job_6ff7f1a7c400916a                      Senior Program Budget Analyst   \n",
            "2  job_e059d20eba88b17a  Senior AI Engineer - Multi-year CONTRACT ROLE ...   \n",
            "\n",
            "          Jobs_Group Profile  Remote  \\\n",
            "0   Business Analyst    Lead  Remote   \n",
            "1  Financial Analyst  Senior     NaN   \n",
            "2     ML/AI Engineer  Senior  Hybrid   \n",
            "\n",
            "                                        Company                   Location  \\\n",
            "0               Dryden Mutual Insurance Company           Dryden, NY 13053   \n",
            "1                              Esphera Concepts  Washington, DC+1 location   \n",
            "2  Volkswagen Group of America - Chattanooga...                Belmont, CA   \n",
            "\n",
            "         City State Frecuency_Salary  \\\n",
            "0      Dryden    NY             year   \n",
            "1  Washington    DC             year   \n",
            "2     Belmont    CA             year   \n",
            "\n",
            "                                              Skills  \\\n",
            "0         ['PowerPoint', 'Office', 'Excel', 'Agile']   \n",
            "1  ['PowerPoint', 'Master', 'Word', 'Bachelor', '...   \n",
            "2  ['Tensor Flow', 'Python', 'Master', 'PhD', 'C+...   \n",
            "\n",
            "                                         Sector Sector_Group Revenue Employee  \\\n",
            "0  Insurance Companies and Investment Societies    Insurance     NaN      NaN   \n",
            "1                     Management and Consulting   Consulting     NaN       XS   \n",
            "2                                           NaN          NaN     NaN      NaN   \n",
            "\n",
            "   Company_Score  Reviews Director  Director_Score  \\\n",
            "0            5.0      2.0      NaN             NaN   \n",
            "1            NaN      NaN      NaN             NaN   \n",
            "2            NaN      NaN      NaN             NaN   \n",
            "\n",
            "                                URL  \n",
            "0                               NaN  \n",
            "1  https://www.espheraconcepts.com/  \n",
            "2                               NaN  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/usjobs_train.csv\")\n",
        "test_df = pd.read_csv(\"/content/usjobs_test.csv\")\n",
        "\n",
        "# Preview the data\n",
        "print(\"Train Shape:\", train_df.shape)\n",
        "print(\"Test Shape:\", test_df.shape)\n",
        "\n",
        "# Show missing value percentages\n",
        "missing_train = train_df.isnull().mean().sort_values(ascending=False)\n",
        "missing_test = test_df.isnull().mean().sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nMissing values in Train:\\n\", missing_train)\n",
        "print(\"\\nMissing values in Test:\\n\", missing_test)\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\nTrain Sample:\\n\", train_df.head(3))\n",
        "print(\"\\nTest Sample:\\n\", test_df.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_drop = ['Director', 'Director_Score', 'URL', 'Profile']\n",
        "\n",
        "train_df = train_df.drop(columns=cols_to_drop)\n",
        "test_df = test_df.drop(columns=cols_to_drop)"
      ],
      "metadata": {
        "id": "X3gKF-hENuGS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter train data for yearly salaries only\n",
        "train_df = train_df[train_df['Frecuency_Salary'] == 'year']\n",
        "test_df = test_df[test_df['Frecuency_Salary'] == 'year']"
      ],
      "metadata": {
        "id": "_fXxOvM0Nu0O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing categorical values\n",
        "categorical_cols = ['Remote', 'Sector', 'Sector_Group', 'Revenue', 'Employee', 'City', 'State', 'Company']\n",
        "for col in categorical_cols:\n",
        "    train_df[col] = train_df[col].fillna('Unknown')\n",
        "    test_df[col] = test_df[col].fillna('Unknown')\n",
        "\n",
        "# Fill missing numeric values\n",
        "numeric_cols = ['Company_Score', 'Reviews']\n",
        "for col in numeric_cols:\n",
        "    train_df[col] = train_df[col].fillna(train_df[col].median())\n",
        "    test_df[col] = test_df[col].fillna(test_df[col].median())"
      ],
      "metadata": {
        "id": "NYH8qHlyNw3L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop only columns that exist\n",
        "cols_to_drop = ['Director', 'Director_Score', 'URL', 'Profile']\n",
        "existing_cols_to_drop = [col for col in cols_to_drop if col in train_df.columns]\n",
        "\n",
        "train_df = train_df.drop(columns=existing_cols_to_drop)\n",
        "test_df = test_df.drop(columns=existing_cols_to_drop)"
      ],
      "metadata": {
        "id": "ua7cQRjFN2XU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Step 1: One-hot encode categorical features\n",
        "categorical_cols = ['Remote', 'Revenue', 'Employee', 'Sector', 'Sector_Group', 'State', 'Jobs_Group']\n",
        "all_df = pd.concat([train_df, test_df], axis=0)\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "encoded_cat = pd.DataFrame(ohe.fit_transform(all_df[categorical_cols]), columns=ohe.get_feature_names_out(categorical_cols))\n",
        "encoded_cat.index = all_df.index\n",
        "all_df = pd.concat([all_df.drop(columns=categorical_cols), encoded_cat], axis=1)"
      ],
      "metadata": {
        "id": "YJVVUxX7OE3W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from collections import Counter\n",
        "\n",
        "# Convert stringified lists to real lists\n",
        "all_df['Skills'] = all_df['Skills'].apply(ast.literal_eval)\n",
        "\n",
        "# Count top 20 most common skills\n",
        "skill_counts = Counter(skill for skills in all_df['Skills'] for skill in skills)\n",
        "top_skills = [skill for skill, count in skill_counts.most_common(20)]\n",
        "\n",
        "# Create binary features for top skills\n",
        "for skill in top_skills:\n",
        "    all_df[f'skill_{skill}'] = all_df['Skills'].apply(lambda x: int(skill in x))\n",
        "\n",
        "# Drop original Skills column\n",
        "all_df = all_df.drop(columns=['Skills'])"
      ],
      "metadata": {
        "id": "Ajqsl9YJOWJw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-separate train and test\n",
        "train_final = all_df[all_df['Mean_Salary'].notnull()]\n",
        "test_final = all_df[all_df['Mean_Salary'].isnull()]\n",
        "\n",
        "# Define features and target\n",
        "X = train_final.drop(columns=['ID', 'Job', 'Company', 'Location', 'City', 'Mean_Salary', 'Frecuency_Salary'])\n",
        "y = train_final['Mean_Salary']\n",
        "X_test = test_final.drop(columns=['ID', 'Job', 'Company', 'Location', 'City', 'Mean_Salary', 'Frecuency_Salary'])"
      ],
      "metadata": {
        "id": "peKBoSGHOklj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import ast\n",
        "from collections import Counter\n",
        "import sklearn\n",
        "\n",
        "# Step 1: One-hot encode categorical features (version-safe)\n",
        "categorical_cols = ['Remote', 'Revenue', 'Employee', 'Sector', 'Sector_Group', 'State', 'Jobs_Group']\n",
        "\n",
        "# Combine train and test\n",
        "all_df = pd.concat([train_df, test_df], axis=0)\n",
        "\n",
        "# Create encoder with correct parameters based on sklearn version\n",
        "if sklearn.__version__ >= '1.2':\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "else:\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "# Fit and transform\n",
        "encoded_cat = pd.DataFrame(ohe.fit_transform(all_df[categorical_cols]),\n",
        "                           columns=ohe.get_feature_names_out(categorical_cols))\n",
        "encoded_cat.index = all_df.index\n",
        "\n",
        "# Add back to dataframe\n",
        "all_df = pd.concat([all_df.drop(columns=categorical_cols), encoded_cat], axis=1)\n",
        "\n",
        "# Step 2: Process Skills column\n",
        "# Convert string lists to real Python lists\n",
        "all_df['Skills'] = all_df['Skills'].apply(ast.literal_eval)\n",
        "\n",
        "# Count top 20 skills\n",
        "skill_counts = Counter(skill for skills in all_df['Skills'] for skill in skills)\n",
        "top_skills = [skill for skill, count in skill_counts.most_common(20)]\n",
        "\n",
        "# Create binary feature for each top skill\n",
        "for skill in top_skills:\n",
        "    all_df[f'skill_{skill}'] = all_df['Skills'].apply(lambda x: int(skill in x))\n",
        "\n",
        "# Drop original skills column\n",
        "all_df = all_df.drop(columns=['Skills'])\n",
        "\n",
        "# Step 3: Re-split datasets\n",
        "train_final = all_df[all_df['Mean_Salary'].notnull()]\n",
        "test_final = all_df[all_df['Mean_Salary'].isnull()]\n",
        "\n",
        "# Features and target\n",
        "X = train_final.drop(columns=['ID', 'Job', 'Company', 'Location', 'City', 'Mean_Salary', 'Frecuency_Salary'])\n",
        "y = train_final['Mean_Salary']\n",
        "X_test = test_final.drop(columns=['ID', 'Job', 'Company', 'Location', 'City', 'Mean_Salary', 'Frecuency_Salary'])"
      ],
      "metadata": {
        "id": "SXvzZL8iOnoh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Initialize model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Evaluate model using cross-validation (RÂ² Score)\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "print(f'Cross-validated RÂ² scores: {scores}')\n",
        "print(f'Mean RÂ² score: {scores.mean():.4f}')\n",
        "\n",
        "# Fit the model on the full training data\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "oYuMz8UOOq3H",
        "outputId": "0479c824-ca5d-4507-9d40-347dd9cf72a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-2082718825.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Evaluate model using cross-validation (RÂ² Score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Cross-validated RÂ² scores: {scores}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Mean RÂ² score: {scores.mean():.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict salary for test data\n",
        "test_preds = model.predict(X_test)"
      ],
      "metadata": {
        "id": "QQNEF8BnPI1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original test_df shape:\", test_df.shape)\n",
        "print(\"Processed test_final shape:\", test_final.shape)"
      ],
      "metadata": {
        "id": "CgkODXCrQME-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Skills in train_df:\", 'Skills' in train_df.columns)\n",
        "print(\"Skills in test_df:\", 'Skills' in test_df.columns)\n",
        "print(\"Skills in all_df:\", 'Skills' in all_df.columns)"
      ],
      "metadata": {
        "id": "j3PoI5k2Qg-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Safe parsing of 'Skills'\n",
        "def safe_parse(x):\n",
        "    try:\n",
        "        return ast.literal_eval(x)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Apply parsing to both dataframes\n",
        "train_df['Skills'] = train_df['Skills'].fillna(\"[]\").apply(safe_parse)\n",
        "test_df['Skills'] = test_df['Skills'].fillna(\"[]\").apply(safe_parse)"
      ],
      "metadata": {
        "id": "ZVh0NkNNQp2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "u-GCTakwQwJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count all skills\n",
        "skill_counter = Counter()\n",
        "for skills in all_df['Skills']:\n",
        "    skill_counter.update(skills)\n",
        "\n",
        "# Use top 100 frequent skills (you can change this number)\n",
        "top_skills = [skill for skill, _ in skill_counter.most_common(100)]\n",
        "\n",
        "# Create binary features for top skills\n",
        "for skill in top_skills:\n",
        "    all_df[f'skill_{skill}'] = all_df['Skills'].apply(lambda x: int(skill in x))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_c31vsfUQy_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.drop(columns=['Skills'], inplace=True)"
      ],
      "metadata": {
        "id": "0KGhjfqKQ0t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final = all_df.iloc[:len(train_df)].copy()\n",
        "test_final = all_df.iloc[len(train_df):].copy()\n",
        "\n",
        "X = train_final.drop(columns=['Mean_Salary', 'ID'])\n",
        "y = train_final['Mean_Salary']\n",
        "X_test = test_final.drop(columns=['ID'])"
      ],
      "metadata": {
        "id": "fLvoLrMHQ4em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show non-numeric columns in X\n",
        "non_numeric_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "print(\"Non-numeric columns in X:\", non_numeric_cols)"
      ],
      "metadata": {
        "id": "Rejvg1lfQ9H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.drop(columns=['Job', 'Company', 'Location', 'City'])\n",
        "X_test = X_test.drop(columns=['Job', 'Company', 'Location', 'City'])"
      ],
      "metadata": {
        "id": "u6OMlTfZRLT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Concatenate train and test again to ensure consistent encoding\n",
        "combined = pd.concat([X, X_test], axis=0)\n",
        "\n",
        "# One-hot encode remaining categorical columns\n",
        "categorical_cols = combined.select_dtypes(include=['object']).columns.tolist()\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)  # for scikit-learn >=1.2 use `sparse_output`\n",
        "\n",
        "encoded = pd.DataFrame(ohe.fit_transform(combined[categorical_cols]), columns=ohe.get_feature_names_out(categorical_cols))\n",
        "\n",
        "# Merge back with numeric features\n",
        "combined = combined.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "encoded = encoded.reset_index(drop=True)\n",
        "final_data = pd.concat([combined, encoded], axis=1)\n",
        "\n",
        "# Re-split\n",
        "X = final_data.iloc[:len(X)]\n",
        "X_test = final_data.iloc[len(X):]"
      ],
      "metadata": {
        "id": "q-sxDu92ROZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "test_preds = model.predict(X_test)"
      ],
      "metadata": {
        "id": "-wpmnWP6RRIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original test_df shape:\", test_df.shape)\n",
        "print(\"Processed X_test shape:\", X_test.shape)\n",
        "print(\"Length of predictions:\", len(test_preds))\n",
        "print(\"Submission shape:\", submission.shape)"
      ],
      "metadata": {
        "id": "N1wFf76pR3bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before any cleaning\n",
        "test_ids = test_df['ID'].copy()"
      ],
      "metadata": {
        "id": "z7a_DphDR4M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ids = test_df['ID'].copy()"
      ],
      "metadata": {
        "id": "db6J0_wtR9iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.fillna(-1, inplace=True)\n",
        "test_df.fillna(-1, inplace=True)"
      ],
      "metadata": {
        "id": "Me2Uu_PrSHX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = model.predict(X_test)"
      ],
      "metadata": {
        "id": "I5MyRDpfSKQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sample submission\n",
        "submission = pd.read_csv('/content/usjobs_sample_submission.csv')\n",
        "\n",
        "# Reset test_ids to align with X_test\n",
        "test_ids = test_df['ID'].reset_index(drop=True)\n",
        "\n",
        "# Reset predictions to align with test_ids\n",
        "preds_df = pd.DataFrame({\n",
        "    'ID': test_ids,\n",
        "    'Mean_Salary': test_preds\n",
        "})\n",
        "\n",
        "# Merge with submission on 'ID'\n",
        "submission = submission.drop(columns=['Mean_Salary'], errors='ignore')\n",
        "submission = submission.merge(preds_df, on='ID', how='left')\n",
        "\n",
        "# Fill any missing predictions with median\n",
        "submission['Mean_Salary'] = submission['Mean_Salary'].fillna(submission['Mean_Salary'].median())\n",
        "\n",
        "# Save final submission\n",
        "submission.to_csv('usjobs_final_submission.csv', index=False)\n",
        "\n",
        "print(\"â Submission file saved: usjobs_final_submission.csv\")"
      ],
      "metadata": {
        "id": "hS4I7AB5SQA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "from math import sqrt\n",
        "\n",
        "# ð Load the data\n",
        "train_df = pd.read_csv('/content/usjobs_train.csv')\n",
        "test_df = pd.read_csv('/content/usjobs_test.csv')\n",
        "submission = pd.read_csv('/content/usjobs_sample_submission.csv')\n",
        "\n",
        "# ð§¹ Drop sparse or unhelpful columns\n",
        "cols_to_drop = ['Director', 'Director_Score', 'URL', 'Profile']\n",
        "train_df = train_df.drop(columns=[col for col in cols_to_drop if col in train_df.columns])\n",
        "test_df = test_df.drop(columns=[col for col in cols_to_drop if col in test_df.columns])\n",
        "\n",
        "# ð Combine for consistent preprocessing\n",
        "all_df = pd.concat([train_df.drop(columns=['Mean_Salary']), test_df], axis=0).reset_index(drop=True)\n",
        "\n",
        "# ð Fill missing categorical values\n",
        "categorical_cols = ['Remote', 'Revenue', 'Employee', 'Sector', 'Sector_Group', 'State', 'Jobs_Group']\n",
        "for col in categorical_cols:\n",
        "    all_df[col] = all_df[col].fillna('Unknown')\n",
        "\n",
        "# ð§  One-Hot Encoding\n",
        "ohe = OneHotEncoder(handle_unknown='ignore')\n",
        "encoded_cat = ohe.fit_transform(all_df[categorical_cols])\n",
        "\n",
        "# ð§¾ TF-IDF on Skills\n",
        "all_df['Skills'] = all_df['Skills'].fillna('')\n",
        "tfidf = TfidfVectorizer(max_features=100)\n",
        "skills_tfidf = tfidf.fit_transform(all_df['Skills'])\n",
        "\n",
        "# ð§  Feature Matrix\n",
        "X_all = hstack([encoded_cat, skills_tfidf])\n",
        "\n",
        "# ð Split back\n",
        "X_train = X_all[:len(train_df)]\n",
        "X_test = X_all[len(train_df):]\n",
        "y = train_df['Mean_Salary']\n",
        "\n",
        "# ð§ª Validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ð² Model Training\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ð Validation Performance\n",
        "val_preds = model.predict(X_val)\n",
        "rmse = sqrt(mean_squared_error(y_val, val_preds))\n",
        "print(f\"Validation RMSE: {rmse:.2f}\")\n",
        "\n",
        "# ð§ª Predict on Test\n",
        "test_preds = model.predict(X_test)\n",
        "\n",
        "# ð¤ Prepare Submission\n",
        "if len(test_preds) != len(submission):\n",
        "    print(\"Mismatch! Adjusting prediction length...\")\n",
        "    test_preds = test_preds[:len(submission)]  # Safe trim\n",
        "\n",
        "submission['Mean_Salary'] = test_preds\n",
        "submission.to_csv('usjobs_final_submission.csv', index=False)\n",
        "print(\"â Submission file 'usjobs_final_submission.csv' is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiKGWsRESaxR",
        "outputId": "a9e6fe8b-5d9d-4118-b224-5064b773fefa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE: 32050.41\n",
            "â Submission file 'usjobs_final_submission.csv' is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit scikit-learn pandas scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CepVaUt1S1VT",
        "outputId": "8ff8dd06-1c94-48b0-e9d8-ace24e953fa1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.47.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.47.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Downloading streamlit-1.47.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.47.0 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# After training...\n",
        "pickle.dump(model, open('rf_model.pkl', 'wb'))\n",
        "pickle.dump(ohe, open('ohe_encoder.pkl', 'wb'))\n",
        "pickle.dump(tfidf, open('tfidf_vectorizer.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "O-LfL4MUVMjf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# --- Load Pretrained Objects ---\n",
        "@st.cache_resource\n",
        "def load_model_and_encoders():\n",
        "    # Load model\n",
        "    model = pickle.load(open('rf_model.pkl', 'rb'))\n",
        "    ohe = pickle.load(open('ohe_encoder.pkl', 'rb'))\n",
        "    tfidf = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "    return model, ohe, tfidf\n",
        "\n",
        "model, ohe, tfidf = load_model_and_encoders()\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.title(\"ð¼ US Job Salary Predictor\")\n",
        "st.markdown(\"Enter job details to estimate the Mean Salary (USD).\")\n",
        "\n",
        "# --- Input Fields ---\n",
        "remote = st.selectbox(\"Remote\", ['Yes', 'No', 'Unknown'])\n",
        "revenue = st.selectbox(\"Company Revenue\", ['Unknown', '<$1M', '$1M-$10M', '$10M-$100M', '$100M-$1B', '>$1B'])\n",
        "employee = st.selectbox(\"Employee Size\", ['Unknown', '1-10', '11-50', '51-200', '201-500', '501-1000', '1001-5000', '5001-10000', '10000+'])\n",
        "sector = st.selectbox(\"Sector\", ['Information Technology', 'Finance', 'Healthcare', 'Retail', 'Manufacturing', 'Unknown'])\n",
        "sector_group = st.selectbox(\"Sector Group\", ['Tech', 'Business', 'Healthcare', 'Unknown'])\n",
        "state = st.selectbox(\"Job State\", ['CA', 'NY', 'TX', 'FL', 'IL', 'Remote', 'Unknown'])\n",
        "jobs_group = st.selectbox(\"Job Group\", ['Engineering', 'Marketing', 'Sales', 'Operations', 'Unknown'])\n",
        "skills = st.text_area(\"Skills / Keywords\", \"Python, SQL, Excel\")\n",
        "\n",
        "# --- Predict Button ---\n",
        "if st.button(\"Predict Salary\"):\n",
        "    # Format input\n",
        "    input_df = pd.DataFrame([{\n",
        "        'Remote': remote,\n",
        "        'Revenue': revenue,\n",
        "        'Employee': employee,\n",
        "        'Sector': sector,\n",
        "        'Sector_Group': sector_group,\n",
        "        'State': state,\n",
        "        'Jobs_Group': jobs_group,\n",
        "        'Skills': skills\n",
        "    }])\n",
        "\n",
        "    # One-hot encode categoricals\n",
        "    cat_features = ohe.transform(input_df[['Remote', 'Revenue', 'Employee', 'Sector', 'Sector_Group', 'State', 'Jobs_Group']])\n",
        "\n",
        "    # TF-IDF for Skills\n",
        "    skill_features = tfidf.transform(input_df['Skills'])\n",
        "\n",
        "    # Combine features\n",
        "    final_input = hstack([cat_features, skill_features])\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(final_input)[0]\n",
        "    st.success(f\"ð° Estimated Mean Salary: **${prediction:,.2f} USD**\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pGn1sc6AU-z9",
        "outputId": "9e321785-2b65-4594-edbc-bf7142dffa42"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-23 13:32:31.193 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.269 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-07-23 13:32:31.269 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.270 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.271 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.499 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.500 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.502 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.503 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.503 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.505 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.506 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.507 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.509 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.510 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.511 Session state does not function when running a script without `streamlit run`\n",
            "2025-07-23 13:32:31.512 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.513 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.515 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.515 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.516 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.517 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.518 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.518 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.521 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.521 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.522 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.523 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.523 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.524 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.525 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.527 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.528 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.528 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.532 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.532 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.533 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.534 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.535 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.535 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.537 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.538 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.538 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.539 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.541 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.541 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.550 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.556 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.557 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.559 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.560 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.563 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.564 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.564 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.565 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.566 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.567 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-23 13:32:31.568 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile my_app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# --- Load Pretrained Objects ---\n",
        "@st.cache_resource\n",
        "def load_model_and_encoders():\n",
        "    # Load model\n",
        "    model = pickle.load(open('rf_model.pkl', 'rb'))\n",
        "    ohe = pickle.load(open('ohe_encoder.pkl', 'rb'))\n",
        "    tfidf = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
        "    return model, ohe, tfidf\n",
        "\n",
        "model, ohe, tfidf = load_model_and_encoders()\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.title(\"ð¼ US Job Salary Predictor\")\n",
        "st.markdown(\"Enter job details to estimate the Mean Salary (USD).\")\n",
        "\n",
        "# --- Input Fields ---\n",
        "remote = st.selectbox(\"Remote\", ['Yes', 'No', 'Unknown'])\n",
        "revenue = st.selectbox(\"Company Revenue\", ['Unknown', '<$1M', '$1M-$10M', '$10M-$100M', '$100M-$1B', '>$1B'])\n",
        "employee = st.selectbox(\"Employee Size\", ['Unknown', '1-10', '11-50', '51-200', '201-500', '501-1000', '1001-5000', '5001-10000', '10000+'])\n",
        "sector = st.selectbox(\"Sector\", ['Information Technology', 'Finance', 'Healthcare', 'Retail', 'Manufacturing', 'Unknown'])\n",
        "sector_group = st.selectbox(\"Sector Group\", ['Tech', 'Business', 'Healthcare', 'Unknown'])\n",
        "state = st.selectbox(\"Job State\", ['CA', 'NY', 'TX', 'FL', 'IL', 'Remote', 'Unknown'])\n",
        "jobs_group = st.selectbox(\"Job Group\", ['Engineering', 'Marketing', 'Sales', 'Operations', 'Unknown'])\n",
        "skills = st.text_area(\"Skills / Keywords\", \"Python, SQL, Excel\")\n",
        "\n",
        "# --- Predict Button ---\n",
        "if st.button(\"Predict Salary\"):\n",
        "    # Format input\n",
        "    input_df = pd.DataFrame([{\n",
        "        'Remote': remote,\n",
        "        'Revenue': revenue,\n",
        "        'Employee': employee,\n",
        "        'Sector': sector,\n",
        "        'Sector_Group': sector_group,\n",
        "        'State': state,\n",
        "        'Jobs_Group': jobs_group,\n",
        "        'Skills': skills\n",
        "    }])\n",
        "\n",
        "    # One-hot encode categoricals\n",
        "    cat_features = ohe.transform(input_df[['Remote', 'Revenue', 'Employee', 'Sector', 'Sector_Group', 'State', 'Jobs_Group']])\n",
        "\n",
        "    # TF-IDF for Skills\n",
        "    skill_features = tfidf.transform(input_df['Skills'])\n",
        "\n",
        "    # Combine features\n",
        "    final_input = hstack([cat_features, skill_features])\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(final_input)[0]\n",
        "    st.success(f\"ð° Estimated Mean Salary: **${prediction:,.2f} USD**\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L7O58U2VGRi",
        "outputId": "4e7c50d6-e735-45ea-aa76-752415bd3007"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-AGPU_2VjkQ",
        "outputId": "51df5bb1-da32-49d4-8305-f3bb0cf64314"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.230.121.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ghfhmTcAVoHS",
        "outputId": "fb75d178-3385-44b4-ca5d-2d3c6110d833"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\n",
            "added 22 packages in 2s\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm audit fix --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gSICHekqVqHU",
        "outputId": "47c85120-c7dc-43fe-aaa1-a7220d50b45d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94musing --force\u001b[39m Recommended protections disabled.\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94maudit\u001b[39m Updating localtunnel to 1.8.3, which is a SemVer major change.\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m cryptiles@2.0.5: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m sntp@1.0.9: This module moved to @hapi/sntp. Please make sure to switch over as this distribution is no longer supported and may contain bugs and critical security issues.\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m har-validator@4.2.1: this library is no longer supported\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m uuid@3.4.0: Please upgrade  to version 7 or higher.  Older versions may use Math.random() in certain circumstances, which is known to be problematic.  See https://v8.dev/blog/math-random for details.\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m boom@2.10.1: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m hoek@2.16.3: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). Please upgrade to the latest version to get the best features, bug fixes, and security patches. If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m request@2.81.0: request has been deprecated, see https://github.com/request/request/issues/3142\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m hawk@3.1.3: This module moved to @hapi/hawk. Please make sure to switch over as this distribution is no longer supported and may contain bugs and critical security issues.\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\n",
            "added 83 packages, removed 10 packages, changed 11 packages, and audited 96 packages in 6s\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K11 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\n",
            "\u001b[1m# npm audit report\u001b[22m\n",
            "\n",
            "\u001b[1majv\u001b[22m  <6.12.3\n",
            "Severity: \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m\n",
            "\u001b[1mPrototype Pollution in Ajv\u001b[22m - https://github.com/advisories/GHSA-v88g-cgmw-v5xw\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/ajv\u001b[22m\n",
            "  \u001b[1mhar-validator\u001b[22m  3.3.0 - 5.1.0\n",
            "  Depends on vulnerable versions of \u001b[1majv\u001b[22m\n",
            "  \u001b[2mnode_modules/har-validator\u001b[22m\n",
            "    \u001b[1mrequest\u001b[22m  *\n",
            "    Depends on vulnerable versions of \u001b[1mform-data\u001b[22m\n",
            "    Depends on vulnerable versions of \u001b[1mhar-validator\u001b[22m\n",
            "    Depends on vulnerable versions of \u001b[1mhawk\u001b[22m\n",
            "    Depends on vulnerable versions of \u001b[1mtough-cookie\u001b[22m\n",
            "    \u001b[2mnode_modules/request\u001b[22m\n",
            "      \u001b[1mlocaltunnel\u001b[22m  <=1.9.0\n",
            "      Depends on vulnerable versions of \u001b[1mdebug\u001b[22m\n",
            "      Depends on vulnerable versions of \u001b[1mrequest\u001b[22m\n",
            "      \u001b[2mnode_modules/localtunnel\u001b[22m\n",
            "\n",
            "\u001b[1mdebug\u001b[22m  <=2.6.8\n",
            "Severity: \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m\n",
            "\u001b[1mdebug Inefficient Regular Expression Complexity vulnerability\u001b[22m - https://github.com/advisories/GHSA-9vvw-cc9w-f27h\n",
            "\u001b[1mRegular Expression Denial of Service in debug\u001b[22m - https://github.com/advisories/GHSA-gxpj-cx7g-858c\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/debug\u001b[22m\n",
            "\n",
            "\u001b[1mform-data\u001b[22m  <2.5.4\n",
            "Severity: \u001b[35m\u001b[1mcritical\u001b[22m\u001b[39m\n",
            "\u001b[1mform-data uses unsafe random function in form-data for choosing boundary\u001b[22m - https://github.com/advisories/GHSA-fjxv-7rqg-78g4\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/form-data\u001b[22m\n",
            "\n",
            "\u001b[1mhawk\u001b[22m  <=9.0.0\n",
            "Severity: \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m\n",
            "\u001b[1mUncontrolled Resource Consumption in Hawk\u001b[22m - https://github.com/advisories/GHSA-44pw-h2cw-w3vq\n",
            "Depends on vulnerable versions of \u001b[1mboom\u001b[22m\n",
            "Depends on vulnerable versions of \u001b[1mcryptiles\u001b[22m\n",
            "Depends on vulnerable versions of \u001b[1mhoek\u001b[22m\n",
            "Depends on vulnerable versions of \u001b[1msntp\u001b[22m\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/hawk\u001b[22m\n",
            "\n",
            "\u001b[1mhoek\u001b[22m  *\n",
            "Severity: \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m\n",
            "\u001b[1mhoek subject to prototype pollution via the clone function.\u001b[22m - https://github.com/advisories/GHSA-c429-5p7v-vgjp\n",
            "\u001b[1mPrototype Pollution in hoek\u001b[22m - https://github.com/advisories/GHSA-jp4x-w63m-7wgm\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/hoek\u001b[22m\n",
            "  \u001b[1mboom\u001b[22m  *\n",
            "  Depends on vulnerable versions of \u001b[1mhoek\u001b[22m\n",
            "  \u001b[2mnode_modules/boom\u001b[22m\n",
            "    \u001b[1mcryptiles\u001b[22m  *\n",
            "    Depends on vulnerable versions of \u001b[1mboom\u001b[22m\n",
            "    \u001b[2mnode_modules/cryptiles\u001b[22m\n",
            "  \u001b[1msntp\u001b[22m  0.0.0 || >=0.1.1\n",
            "  Depends on vulnerable versions of \u001b[1mhoek\u001b[22m\n",
            "  \u001b[2mnode_modules/sntp\u001b[22m\n",
            "\n",
            "\n",
            "\u001b[1mtough-cookie\u001b[22m  <4.1.3\n",
            "Severity: \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m\n",
            "\u001b[1mtough-cookie Prototype Pollution vulnerability\u001b[22m - https://github.com/advisories/GHSA-72xf-g2v4-qvf3\n",
            "\u001b[32m\u001b[1mfix available\u001b[22m\u001b[39m via `npm audit fix`\n",
            "\u001b[2mnode_modules/tough-cookie\u001b[22m\n",
            "\n",
            "\u001b[31m\u001b[1m12\u001b[22m\u001b[39m vulnerabilities (3 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m, 7 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m, 2 \u001b[35m\u001b[1mcritical\u001b[22m\u001b[39m)\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run my_app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UafB101VsVF",
        "outputId": "4a3ebce8-69f8-4e95-c9da-b7556748145d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0K\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.230.121.69:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://every-peas-think.loca.lt\n"
          ]
        }
      ]
    }
  ]
}